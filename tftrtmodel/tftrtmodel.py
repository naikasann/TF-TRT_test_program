import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.models import load_model, model_from_json, model_from_yaml
from tensorflow.python import keras
from tensorflow.python.compiler import tensorrt
from tensorflow.python.compiler.tensorrt import trt_convert as trt
from tensorflow.python.saved_model import tag_constants
import numpy as np
import os
import shutil

class TFTRTmodel:
    # Declare a location where model instances can be stored so that model inputs, etc.
    # can be referenced from outside.
    infer = None
    # Calling parameters of tensorRT conversion.
    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS

    def __init__(self,
                 tensorrt_model:str,
                 precision_mode="fp32",
                 input_model="",
                 batch_size=1,
                 max_workspace_size_bytes=8000000000,
                 maximum_cached_engines=10000,
                 weight_path="") -> None:
        self.batch_size = batch_size
        """
            Generate or load the model of TF-TRT and keep it in the model instance.
            Basically, by calling this, you can complete the preparation of TF-TRT.
            It controls whether the TF-TRT model is generated or not depending on whether
            input_model exists or not and whether the output model exists or not.

			args 	:
                tensorrt_model:str - Specify the model path of TF-TRT.
                                     (If model exists -> load it.
                                     If not -> generate it at the specified location.)
                precision_mode:str - Adjusts the numerical accuracy of the model to be generated by TF-TRT.
                                     The smaller the number, the faster the inference is expected to be.
                                     support : int8, fp16, fp32
                input_model:str="" - Specify the model path to convert to TF-TRT model.
                                     WARM!   : If the location to specify the TF-TRT model already has some files,
                                               it will be loaded without generating them.
                                     support : tensorflow asset model(.pb)
                batch_size:int=1   - Used to calibrate when the numerical precision is set to int8.
                max_workspace_size_bytes - Specifies the maximum GPU memory size that can be used by TensorRT.
                                           The larger the number, the quicker the inference will be performed. Too much will result in an error.
                maximum_cached_engines - The maximum number of TensorRT engines cached for dynamic TensorRT operations.
                weight_path:str        - If you want to load weights from a keras model, put the path to the weights here
			return 	: None
		"""
        # If there is no TF-TRTmodel, perform the generation process.
        if not os.path.exists(tensorrt_model):
            print("TF-TRT model create...")
            self.infer = self.convert_model_from_tf2trt(precision_mode,
                                                        input_model_name=input_model,
                                                        output_model_name=tensorrt_model,
                                                        max_workspace_size_bytes=max_workspace_size_bytes,
                                                        maximum_cached_engines=maximum_cached_engines,
                                                        weight_path=weight_path)
        # Load TF-TRTmodel if it exists.
        print("TF-TRT model is exist.")
        print("Execute the process without generating a model.")
        self.infer = self.load_tensorrt_model(tensorrt_model)


    def calibration_input_fn(self):
        """
            For the calibration process when the numerical precision is int8.
            FIXME : Currently not available, so make it available.

			args 	: None
			return 	: None
		"""
        yield (self.batch_size, )

    def keras2tfmodel(self, model_path, weight_path="", summary=False):
        """
            Since the tensorflow format model is required to generate the TF-TRT model,
            convert the model to be able to use the tensorflow format model
            if it is saved in Keras, etc., and generate a temp file.

			args 	:
                model_path:str  - keras model path
                weight_path:str - Write if you want to load a keras weight file
                summary:bool    - Display the keras model architecture. for debug.
			return 	: None
		"""
        print("keras model -> tf model.")
        # Extract the file extension.
        file_extension = os.path.splitext(keras_model)
        # Load the keras model for each extension
        if file_extension[1]==".json":
            model = model_from_json(open(model_path).read())
        elif file_extension[1]==".yaml" or file_extension[1]==".yml":
            model = model_from_yaml(open(model_path).read())
        elif file_extension[1]==".h5":
            model = load_model(model_path, compile=False)
        # If the file extension is unsupported.
        else:
            raise ValueError("no support model path.")
        # for debug. print model architecture.
        if summary:
            model.summary()
        # When loading weight data for deep learning.
        if not weight_path == "":
            model.load_weights(weight_path)
        # Save a file for temporary storage.
        model.save("temp_model")

    def convert_model_from_tf2trt(self,
                                  precision_mode="fp32",
                                  input_model_name="",
                                  output_model_name="",
                                  max_workspace_size_bytes=8000000000,
                                  maximum_cached_engines=10000,
                                  weight_path="") -> None:
        """
            Run the generation of the TF-TRT model from the tensorflow model.
            It also shapes the converter with the parameters specified in the arguments.
            Usually, no external calls are made.

			args 	: See the init method of the TFTRTmodel class.
			return 	: None
		"""
        print("convert model TF-TRT")
        # Change the parameters to match the type conversion of each inference.
        self.conversion_params._replace(max_workspace_size_bytes=max_workspace_size_bytes)
        self.conversion_params._replace(maximum_cached_engines=maximum_cached_engines)
        if   precision_mode == "fp32":
            self.conversion_params._replace(precision_mode=trt.TrtPrecisionMode.FP32)
        elif precision_mode == "fp16":
            self.conversion_params._replace(precision_mode=trt.TrtPrecisionMode.FP16)
        elif precision_mode == "int8":
            self.conversion_params._replace(precision_mode=trt.TrtPrecisionMode.INT8,
                                        use_calibration=True)

        # Extract the file extension.
        file_extension = os.path.splitext(input_model_name)
        # Since it is not saved as a tensorflow model,
        # check if it is saved as a keras model and convert it to a tensorflow model.
        if not file_extension[1] == "":
            self.keras2tfmodel(input_model_name, weight_path)
            # Make sure to load the tensorflow model temp file
            input_model_name="temp_model"

        # convert model to tensorrt
        converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_model_name,
                                            conversion_params=self.conversion_params)

        if   precision_mode == "fp32" or precision_mode == "fp16":
            converter.convert()
        elif precision_mode == "int8":
            # FIXME : Calibration functions are required. However, when I write it, an error occurs.
            converter.convert()
            # It's supposed to be written like this...
            #converter.convert(calibration_input_fn=self.calibration_input_fn)

        # Delete the temporary waiting file used during the conversion from keras
        if input_model_name == "temp_model":
            shutil.rmtree("temp_model")

        # TF-TRT save model
        converter.save(output_saved_model_dir=output_model_name)

    def load_tensorrt_model(self, model_name=""):
        """
            Called if the TF-TRT model exists.
            Load the TF-TRT model and store it in an instance of the class.
            Usually, no external calls are made.

			args 	: model_name - TF-TRT model path.
			return 	: TF-TRT model
		"""
        print("load TF-TRT model.")
        model = tf.saved_model.load(model_name, tags=[tf.saved_model.SERVING])
        self.infer = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]

        return self.infer

    def predict(self, x, output_layer="predictions", measure_runtime=False):
        """
            The actual inference is performed using the model of TF-TRT held in the class instance.
            Calling it from the outside executes the inference with the TF-TRT model.
            WARN : An error will occur if the inference is not performed after checking the
                   consistency of the output layer names and inputs. Check the status of infer.

			args 	:
                x:any                - Data to be input to TF-TRT
                output_layer:str     - Name of the output layer of the TF-TRT model
                                        (this can be used to retrieve inference data)
                measure_runtime:bool - Measure the time it takes to make inferences.
			return 	:
                y:any                - Value output from the output layer of TF-TRT
                runtime:float        - measure the time takes to inferences.
		"""
        # Is the model loaded?
        if self.infer is None:
            raise TypeError("The tensorrt model has not been loaded.")

        if type(x) == dict:
            # for multi input layer
            #inputs = {"input_1":x}

            # FIXME : I doubt this is the right place for this.
            y = self.infer(**x)[output_layer]
            return y
        else:
            # Perform inference.
            y = self.infer(tf.convert_to_tensor(x))[output_layer]
            return y

if __name__ == "__main__":
    keras_model = ResNet50(weights="imagenet")
    keras_model.save("resnet50")
    tftrt_model = TFTRTmodel(input_model="resnet50",
                             tensorrt_model="resnet50_tensorRT",
                             precision_mode="fp32",
                             batch_size=1,
                             max_workspace_size_byte=8000000000)
    # dammy input
    x = np.random.uniform(size=(3, 224, 224, 3)).astype(np.float32)
    y, runtime = tftrt_model.predict(x, measure_runtime=True)
    print("output : {}, measure time : {}".format(y, runtime))
